ODENet(
  (net_prods): Sequential(
    (activation_0): LogShiftedSoftSignMod()
    (linear_out): Linear(in_features=350, out_features=70, bias=True)
  )
  (net_sums): Sequential(
    (activation_0): SoftsignMod()
    (linear_out): Linear(in_features=350, out_features=70, bias=True)
  )
  (net_alpha_combine): Sequential(
    (linear_out): Linear(in_features=140, out_features=350, bias=False)
  )
)


    def forward(self, t, y):
        sums = self.net_sums(y)
        prods = torch.exp(self.net_prods(y))
        sums_prods_concat = torch.cat((sums, prods), dim= - 1)
        joint = self.net_alpha_combine(sums_prods_concat)
        #final = joint-torch.relu(self.gene_multipliers)*y
        final = torch.relu(self.gene_multipliers)*(joint-y)
        return(final) 

lambda at start (first 5 epochs) = 1
and then lambda = 1
causal lottery!
doing PPI mask + T mask
pruning score lambda (PPI, Motif) = (0.5, 0.5)
Initial hit = 0.7 at epoch 3, then prune 0.1 every 10 epochs

if name in ['net_sums.linear_out', ]: 
    current_NN_weights_abs = abs(module.weight.detach())
    current_NN_weights_abs = current_NN_weights_abs/torch.sum(current_NN_weights_abs) #trying normalization this for PPI layers
    
elif name == 'net_prods.linear_out':
    current_NN_weights_abs = torch.exp(module.weight.detach())
    current_NN_weights_abs = current_NN_weights_abs/torch.sum(current_NN_weights_abs) #trying normalization this for PPI layers
    
elif name == 'net_alpha_combine.linear_out':
    current_NN_weights_abs = abs(module.weight.detach())
    current_NN_weights_abs = current_NN_weights_abs/torch.sum(current_NN_weights_abs)

sums_mask_curr = normalize_values(my_current_custom_pruning_scores['net_sums.linear_out'])
prods_mask_curr = normalize_values(my_current_custom_pruning_scores['net_prods.linear_out'])
combo_mask_curr = torch.vstack((sums_mask_curr, prods_mask_curr))
